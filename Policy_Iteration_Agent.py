"""
Created on Friday Sep 18 10:20:56 CST 2020

@author: Grayson Schaer
"""

import numpy as np
import matplotlib.pyplot as plt
import time

class Policy_Iteration_Agent():
    
    # Agent constructor
    # @param n_s - integer size of the state space
    # @param n_a - integer size of the action space
    # @param n_rows - Size of the state space in the x direction
    # @param n_cols - Size of the state space in the y direction
    # @param gamma - discount ratio
    # @param env - Environment passed that contains model data
    def __init__(self, n_s, n_a, n_rows, n_cols, gamma, env):
        
        #Check to make sure the planar state definition is consistent
        if (n_rows * n_cols != n_s):
            print("FAILED TO CREATED AGENT: PLANAR STATE SPACE SIZE DOES NOT MATCH NUMBER OF STATES.")
            return
        
        # Sets the size of the state space
        self.n_s = n_s
        
        # Sets the size of the action space
        self.n_a = n_a
        
        # Sets the size of the x direction in the state space
        self.n_rows = n_rows
        
        # Sets the size of the y direction in the state space
        self.n_cols = n_cols
        
        # Defines the discounting rate defined by the problem definition
        self.gamma = gamma
        
        # Defines the environment in which agent acts
        self.env = env
        
        # Initializes estimated Value function (updated by policy iteration algorithm)
        self.V = [0.0]*self.n_s
        
        # Initializes estimated Quality function (updated by policy iteration algorithm)
        self.Q = np.zeros((self.n_s, self.n_a))
        
        # Initializes policy (deterministic based on Quality function)
        self.p = np.zeros((self.n_s, self.n_a))
        for i in range(self.n_s):
            self.p[i][np.argmax(self.Q[i])] = 1.0
        
        # Defines the variables used in the log
        self.r_tot = 0
        self.r_tot_discount = 0
        self.curr_step = 0
        
        #Creates a log that tracks the training of the agent
        # @entry avg_value - Average of the value function at the current training step
        self.training_log = {
            'avg_value': [],
            }
        
        # Defines the variables used in the logbook
        self.entry_number = 1
        self.highest_r_tot = -1
        self.last_episode_last_index = -1
        
        # Creates a log that tracks a single episode
        # @entry s - the state at each iteration step
        # @entry a - the action taken at each iteration step
        # @entry r - the reward given at each iteration step
        # @entry r_tot - the sum of all rewards given up to the current iteration step
        # @entry r_tot_discount - the sum of all the discounted rewards given up to the current iteration step
        self.simulation_log = {
            's': [],
            'a': [],
            'r': [],
            'r_tot': [],
            'r_tot_discount': [],
            }
        
        # Creates a book that tracks results from previous episode sets
        # @entry logs - records each log for each set of episodes run by an agent
        # @entry avg_Q - tracks the average Quality function generated by each agent
        # @entry avg_V - tracks the average Value function generated by each agent
        # @entry avg_p - tracks the average policy function generated by each agent
        # @entry alpha - learning rate of episode set
        # @entry gamma - discount factor of episode set
        # @entry epsilon - exploration rate of episode set
        self.logbook = {
            'best_s': [],
            'best_a': [],
            'best_r': [],
            'r_tot_avg': 0,
            'r_tot_discount_avg': 0,
            'gamma': [],
            }
    
    # Public getter for the estimated value function
    # @return estimate value function in planar state space form
    def get_V(self):
        
        # Init planar representation of the Value function
        planar_V = np.zeros((self.n_rows, self.n_cols))
        
        # Assign each cell in planar_V its associated Value function output
        index = 0
        for i in range(self.n_rows):
            for j in range(self.n_cols):
                planar_V[i][j] = self.V[index]
                index += 1
                
        # Return the pretty version of the Value function
        return planar_V
    
    # Public getter for the current policy
    # @return current policy  in planar state space form
    def get_p(self):
        
        # Init planar representation of the policy function
        planar_p = np.zeros((self.n_rows, self.n_cols))
        
        # Assign each cell in planar_p its associated action based on the policy, action_min, and action_space
        index = 0
        for i in range(self.n_rows):
            for j in range(self.n_cols):
                action_index = np.argmax(self.p[index])
                planar_p[i][j] = action_index
                index += 1
        
        # Return the pretty representation of the policy
        return planar_p
    
    # Updates the best trajectory at the end of each episode
    def end_episode(self):
        # Update the best trajectory log
        if self.simulation_log['r_tot'][-1] > self.highest_r_tot:
            self.highest_r_tot = self.simulation_log['r_tot'][-1]
            self.logbook['best_s'] = self.simulation_log['s'][(self.last_episode_last_index + 1):(len(self.simulation_log['r_tot']) - 1)]
            self.logbook['best_a'] = self.simulation_log['a'][(self.last_episode_last_index + 1):(len(self.simulation_log['r_tot']) - 1)]
            self.logbook['best_r'] = self.simulation_log['r'][(self.last_episode_last_index + 1):(len(self.simulation_log['r_tot']) - 1)]
            
        self.last_episode_last_index = len(self.simulation_log['r_tot']) - 1 
    
    # Archives the log from the current episode to the log book. Resets agent to initial conditions
    def terminate_agent(self):
        # Update the average learning curves
        n = self.entry_number
        self.entry_number += 1
        r_tot_avg = (1/n) * ((n - 1) * self.logbook['r_tot_avg'] + np.asarray(self.simulation_log['r_tot']))
        r_tot_discount_avg = (1/n) * ((n - 1) * self.logbook['r_tot_discount_avg'] + np.asarray(self.simulation_log['r_tot_discount']))

        # In the case of the first set, the average value is just the current value
        if n==1:
            r_tot_avg = self.simulation_log['r_tot']
            r_tot_discount_avg = self.simulation_log['r_tot_discount'] 
        
        self.logbook['r_tot_avg'] = r_tot_avg
        self.logbook['r_tot_discount_avg'] = r_tot_discount_avg
        self.logbook['gamma'].append(self.gamma)
        
        # Reset the log
        self.simulation_log = {
            's': [],
            'a': [],
            'r': [],
            'r_tot': [],
            'r_tot_discount': [],
            }
        
        # Defines the variables used in the log
        self.r_tot = 0
        self.r_tot_discount = 0
        self.curr_step = 0
        
    # Solves the value function estimate based on the current policy using the policy iteration algorithm
    # @return Updated training log associated with policy evaluation iterations
    def policy_eval(self):
        
        # Define the value function estimate at iteration k (current iteration)
        Vk = self.V
    
        # Iterate through many values of k (here chosen randomly) to update the value function
        for k in range(50):
            
            # Update every point s in the value function
            for s in range(self.n_s):
                
                # Represents the value function at s for the k+1 iteration
                Vk_p1_s = 0
                
                # Go through all actions
                for a in range (self.n_a):
                    
                    # Init the bellman sum to 0
                    bellman_sum = 0
                    
                    # Go through all possible states
                    for s2 in range(self.n_s):
                        
                        # Update the bellamn sum
                        p = self.env.p(s2, s, a)
                        r = self.env.r(s, a)
                        bellman_sum += p * (r + self.gamma * Vk[s2])
                        
                    # Update the value function estimate for the k+1 iteration at state s via the policy evaluation algorithm
                    Vk_p1_s += self.p[s, a] * bellman_sum
                    
                # Once all actions have been explored for a give state, update that state's value function
                self.V[s] = Vk_p1_s
                
            # Once a single iteration (k) is complete, reset the definition of the current Vk
            Vk = self.V
            
            # Update the training log after each iteration (k)
            self.training_log['avg_value'].append(sum(self.V) / len(self.V))
              
    # Solves the quality function based on the policy iteration algorithm, then uses a greedy algorithm to update the current policy
    def policy_improve(self):
        
        # Solve the quality function based on the current value function
        for s in range(self.n_s):
            
            # Step through all actions
            for a in range(self.n_a):
                
                # init the bellman sum used to evalue the quality function
                bellman_sum = 0
                
                # Step through all prime states given a current state and action
                for s2 in range(self.n_s):
                    
                    # Update the bellamn sum
                    p = self.env.p(s2, s, a)
                    r = self.env.r(s, a)
                    bellman_sum += p * (r + self.gamma * self.V[s2])
                    
                # Once all prime states have been iterated, update the current state action pair in the quality function
                self.Q[s][a] = bellman_sum
                
        # Once the quality function is calculated, update the current policy based on a greedy algorithm
        # Note this step can be done in the previous loop, but is moved to this section for readability
        # Runtime is a myth
        self.p = np.zeros((self.n_s, self.n_a))
        
        # Step through all states and set the best action (based on the quality function) to a probability of 1. 
        # (all other actions are already at a probability of 0)
        for s in range(self.n_s):
            
            # Find and set greedy-action
            greedy_action = np.argmax(self.Q[s])
            self.p[s][greedy_action] = 1
    
    # Trains the agent based on the policy iteration (eval, improve, repeat) algorithm until policy stability
    # @param plot - determines whether the user wants a plot of the log made
    # @param path - save path for plot
    # @return log that shows the average of value function vs policy evaluation steps (k)
    def train(self, plot=True, path = ''):
        
        print("Training Policy Iteration Agent...")
        start = time.time()
        
        # Reset the training log
        self.training_log = {
            'avg_value': [],
            }
        
        # Init previous policy to unobtainable value so that the while loop is executed at least once
        # Use hacky logic to make a fake do-while loop
        previous_policy = -1
        
        # While any of the previous policy entries are not the same as the last
        while (previous_policy != self.p).any():
            
            # record previous policy to check for policy stability
            previous_policy = self.p
            
            # Evaluate policy
            self.policy_eval()
            
            # Improve policy
            self.policy_improve()
            
        # When training is done, if the user wants, plot the training curve
        if plot:
            plt.clf()
            plt.plot([*range(len(self.training_log['avg_value']))], self.training_log['avg_value'])
            plt.xlabel('Iteration Step')
            plt.ylabel('Average of the Value Function')
            title_str = "Policy Iteration Learning Curve: \u03B3 = " + str(self.gamma)
            plt.title(title_str)
            save_path = str(path) + 'pol_val_cur.png'
            plt.savefig(save_path, dpi = 200)
            plt.close()
            
        elapsed = time.time() - start
        print("Value Iteration training took: " + str(elapsed) + " seconds.")
        
        return self.training_log
            
    # Ask for an action based on the current policy and the current state and updates the log
    # @param s1 - The current state
    # @return - The policy determined action
    def get_action(self, s1):
        
        a1 = np.argmax(self.p[s1])
        r = self.env.r(s1, a1)

        #Update the log
        self.r_tot = self.r_tot + r
        self.r_tot_discount = self.r_tot_discount + (self.gamma ** (self.curr_step)) *  r
        self.simulation_log['s'].append(s1)
        self.simulation_log['a'].append(a1)
        self.simulation_log['r'].append(r)
        self.simulation_log['r_tot'].append(self.r_tot)
        self.simulation_log['r_tot_discount'].append(self.r_tot_discount)
        self.curr_step = self.curr_step + 1

        return a1